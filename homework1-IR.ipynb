{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Information Retrieval.\n",
    "\n",
    "\n",
    "Jeroen Baars - 10686320.\n",
    "\n",
    "Bob .\n",
    "\n",
    "Lea ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Evaluation.\n",
    "\n",
    "Deadline: Wednesday, January 17th, midnight (23:59).\n",
    "\n",
    "Collaboration: This is a team-based assignment. Form teams of 3 people.\n",
    "\n",
    "Submit: An IPython Notebook for both the theoretical and the experimental parts with the necessary (a) answers, (b) implementation, (c) explanations,  (d) comments, and (e) analysis. Code quality, informative comments, detailed explanations of what each block in the notebook does and more important convincing analysis of the results will be considered when grading. \n",
    "\n",
    "Submit your work through Blackboard. All students in a team need to submit their work.\n",
    "\n",
    "Filename: < student 1 id >‚Äì< student 2 id >‚Äì< student 3 id >‚Äìhw1.ipynb.\n",
    "\n",
    "The homework will cover the following three topics covered in Lecture 1, 2, and 3:\n",
    "- Evaluation measures;\n",
    "- Interleaving;\n",
    "- Click models.\n",
    "\n",
    "The homework has a small theoretical part and a large experimental part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Part [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1.Hypothesis Testing ‚Äì The problem of multiple comparisons [5 points]\n",
    "Experimentation in AI often happens like this:__\n",
    "- Modify/Build an algorithm\n",
    "- Compare the algorithm to a baseline by running a hypothesis test.\n",
    "- If not significant, go back to step A\n",
    "- If significant, start writing a paper. \n",
    "\n",
    "Compute the probabilities below ~~How many hypothesis tests, m, does it take to get to~~ (with Type I error for each test = Œ±):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A. P(mth experiment gives significant result | m experiments lacking power to reject H0)?__\n",
    "\n",
    "Answer:\n",
    "    \n",
    "mogelijk iets:\n",
    "https://www.stat.berkeley.edu/~mgoldman/Section0402.pdf\n",
    "\n",
    "piazza staat de vraag anders geformuleerd en uitgelegd:\n",
    "\n",
    "The chance of getting a Type I error is present in all experiments and independently it is $\\alpha$ We care about the m-th experiment making an actual mistake. Here is a reformulation of the question:\n",
    "\n",
    " \n",
    "\n",
    "Suppose hypothetically that the null hypothesis is actually true.\n",
    "\n",
    " \n",
    "\n",
    "The probability of concluding it is false after one test is Œ±. If we do not find it false, we run a second experiment.\n",
    "\n",
    "What is the probability of concluding that it is false in this second experiment? If we do not find it false, we run a third experiment. And so on and so forth. If we do not find it wrong after m-1 experiment, we run an m-th one. What is the probability of concluding that it is false in this m-th experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. P(at least one significant result | m experiments lacking power to reject H0)?__\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2. Bias and unfairness in Interleaving experiments [10 points].__\n",
    "\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ‚Öî of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Part [85 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Commercial search engines use both offline and online approach in evaluating a new search algorithm: they first use an offline test collection to compare the production algorithm (P) with the new experimental algorithm (E); if E statistically significantly outperforms P with respect to the evaluation measure of their interest, the two algorithms are then compared online through an interleaving experiment.\n",
    "\n",
    "For the purpose of this homework we will assume that the evaluation measures of interest are:\n",
    "- Binary evaluation measures\n",
    "    - Precision at rank k,\n",
    "    - Recall at rank k,\n",
    "    - Average Precision,\n",
    "- Multi-graded evaluation measures\n",
    "    - Normalized Discounted Cumulative Gain at rank k (nDCG@k),\n",
    "    - Expected Reciprocal Rank (ERR).\n",
    "    \n",
    "Further, for the purpose of this homework we will assume that the interleaving algorithms of interest are:\n",
    "- Team-Draft Interleaving (Joachims. \"Evaluating retrieval performance using clickthrough data\". Text Mining 2003.),\n",
    "- ~~Probabilistic Interleaving (Hofmann, Whiteson, and de Rijke. \"A probabilistic method for inferring preferences from clicks.\" CIKM 2011.).~~\n",
    " \n",
    "In an interleaving experiment the ranked results of P and E (against a user query) are interleaved in a single ranked list which is presented to a user. The user then clicks on the results and the algorithm that receives most of the clicks wins the comparison. The experiment is repeated for a number of times (impressions) and the total wins for P and E are computed. \n",
    "\n",
    "A Sign/Binomial Test is then run to examine whether the difference in wins between the two algorithms is statistically significant (or due to chance). Alternatively one can calculate the proportion of times the E wins and test whether this proportion, p, is greater than p0=0.5. This is called an 1-sample 1-sided proportion test.\n",
    "\n",
    "One of the key questions however is __whether offline evaluation and online evaluation outcomes agree with each other__. In this homework you will determine the degree of agreement between offline evaluation measures and interleaving outcomes, by the means of simulations. A similar analysis using actual online data can be found at Chapelle et al. ‚ÄúLarge-Scale Validation and Analysis of Interleaved Search Evaluation‚Äù.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Based on Lecture 1]\n",
    "\n",
    "__Step 1: Simulate Rankings of Relevance for E and P (5 points).__\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5.<br>\n",
    "\n",
    "This step should give you about.<br>\n",
    "Example:<br>\n",
    "P: {N N N N N}<br>\n",
    "E: {N N N N R}<br>\n",
    "‚Ä¶<br>\n",
    "P: {HR HR HR HR R}<br>\n",
    "E: {HR HR HR HR HR}<br>\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 5000 pair uniformly at random to show your work.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'N', 'R')]\n",
      "[('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'N', 'HR')]\n",
      "[('N', 'N', 'N', 'N', 'N'), ('N', 'N', 'N', 'R', 'N')]\n",
      "[('HR', 'HR', 'HR', 'HR', 'HR'), ('HR', 'HR', 'HR', 'R', 'HR')]\n",
      "[('HR', 'HR', 'HR', 'HR', 'HR'), ('HR', 'HR', 'HR', 'HR', 'N')]\n",
      "[('HR', 'HR', 'HR', 'HR', 'HR'), ('HR', 'HR', 'HR', 'HR', 'R')]\n"
     ]
    }
   ],
   "source": [
    "combis = list(itertools.product(['N','R','HR'], repeat=10))\n",
    "combinations = [[comb[:5],comb[5:]] for comb in combis if comb[:5] != comb[5:]]\n",
    "for i in combinations[0:3]:\n",
    "    print(i)\n",
    "for j in combinations[len(combinations)-3:]:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2: Implement Evaluation Measures (10 points).__\n",
    "\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection ‚Äì pay extra attention on how to find this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3: Calculate the ùõ•measure (0 points).__\n",
    "\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ùõ•measure = measureE-measureP. Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Based on Lecture 2]\n",
    "__Step 4: Implement Interleaving (15 points).__\n",
    "\n",
    "Implement ~~2~~ 1 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, ~~AND (2), Probabilistic Interleaving.~~ The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
